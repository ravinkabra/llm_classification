{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7746032,"sourceType":"datasetVersion","datasetId":4528124}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U bitsandbytes accelerate\n!pip install -q --upgrade transformers","metadata":{"_uuid":"4c028cb8-a9c9-489a-abc7-acbfab6e1a6e","_cell_guid":"66309f38-8126-4021-82c2-5c8bd67f3986","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-11T19:29:57.097106Z","iopub.execute_input":"2025-05-11T19:29:57.097409Z","iopub.status.idle":"2025-05-11T19:30:03.724159Z","shell.execute_reply.started":"2025-05-11T19:29:57.097380Z","shell.execute_reply":"2025-05-11T19:30:03.723214Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n# ✅ Set GPU visibility FIRST\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Must be first\n\n# Then import everything else\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, confusion_matrix\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import (\n    accuracy_score, f1_score,\n    precision_recall_fscore_support,\n    confusion_matrix\n)\n\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    AutoModelForCausalLM,\n    Trainer,\n    TrainingArguments,\n    BitsAndBytesConfig,\n    pipeline\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"dc42662b-56fe-4134-9392-d1a6f90f9aa9","_cell_guid":"f2ecec7c-e723-414a-9890-a3eac229bc29","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-11T19:30:03.728452Z","iopub.execute_input":"2025-05-11T19:30:03.728798Z","iopub.status.idle":"2025-05-11T19:30:12.529775Z","shell.execute_reply.started":"2025-05-11T19:30:03.728774Z","shell.execute_reply":"2025-05-11T19:30:12.528995Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers, sys; print(transformers.__version__)","metadata":{"_uuid":"d8d69982-7a22-4bc8-9772-245630166776","_cell_guid":"92521358-b895-4fde-ae5c-1e118c59c72e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers, inspect, sys, os\nprint(transformers.__version__)          # ⟶ 4.51.3\nprint(inspect.signature(transformers.TrainingArguments))  # shows evaluation_strategy","metadata":{"_uuid":"0f081e66-e658-4458-9c72-14679b43473b","_cell_guid":"33f25bf6-c77a-4466-9f5f-2925d9c80db5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/usr/bin/env python\n\n# 1. Setup device\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# 2. Load and preprocess data\ndf = pd.read_csv(\"/kaggle/input/bank-customer-complaint-analysis/complaints.csv\").drop(columns=[\"Unnamed: 0\"])\nlabel_encoder = LabelEncoder()\ndf[\"product_label\"] = label_encoder.fit_transform(df[\"product\"])\nlabel_mappings = dict(zip(label_encoder.classes_,\n                          label_encoder.transform(label_encoder.classes_)))\nprint(\"Label mappings:\", label_mappings)\n\n# 3. Sample 10% and stratified splits\nsampled = df.sample(frac=0.1, random_state=42)\ntrain_df, temp_df = train_test_split(\n    sampled, test_size=0.2, stratify=sampled[\"product_label\"], random_state=42\n)\nval_df, test_df = train_test_split(\n    temp_df, test_size=0.5, stratify=temp_df[\"product_label\"], random_state=42\n)\n\n# def to_hf(ds):\n#     return Dataset.from_dict({\n#         \"text\": ds[\"narrative\"].tolist(),\n#         \"labels\": ds[\"product_label\"].tolist()\n#     })\ndef to_hf(ds):\n    return Dataset.from_dict({\n        \"text\": ds[\"narrative\"].fillna(\"\").astype(str).tolist(),  # ✅ clean text\n        \"labels\": ds[\"product_label\"].astype(int).tolist()        # ✅ ensure ints\n    })\n\n\ntrain_ds = to_hf(train_df)\nval_ds   = to_hf(val_df)\ntest_ds  = to_hf(test_df)\n\n# 4. Load tokenizer & quantization config\nmodel_id   = \"bert-base-uncased\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# 5. Tokenize datasets\ndef tokenize_fn(ex):\n    return tokenizer(ex[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n\ntrain_ds = train_ds.map(tokenize_fn, batched=True)\nval_ds   = val_ds.map(tokenize_fn, batched=True)\ntest_ds  = test_ds.map(tokenize_fn, batched=True)\n\n# 6. Load BERT classifier + LoRA\nclf = AutoModelForSequenceClassification.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    low_cpu_mem_usage=True,\n    num_labels=len(label_encoder.classes_),\n    torch_dtype=torch.float32,\n)\nlora_cfg = LoraConfig(\n    r=4,\n    lora_alpha=32,\n    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n    task_type=TaskType.SEQ_CLS,\n)\nclf = get_peft_model(clf, lora_cfg)\nclf.to(device)\n\n# 7. Define metrics\ndef compute_metrics(pred):\n    logits, labels = pred\n    preds = logits.argmax(axis=-1)\n    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n\n# 8. Training arguments & Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    fp16=True,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=clf,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n# 9. Train & evaluate\ntrainer.train()\nprint(\"Validation:\", trainer.evaluate())\n\nraw_preds, labels, _ = trainer.predict(test_ds)\npreds = raw_preds.argmax(axis=-1)\n\nprint(f\"Test Accuracy: {accuracy_score(labels, preds):.4f}\")\nprint(f\"Test F1 Score: {f1_score(labels, preds, average='weighted'):.4f}\")\n\n# 10. Confusion matrix\ncm = confusion_matrix(labels, preds)\nplt.figure(figsize=(8, 6))\nsns.heatmap(\n    cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n    xticklabels=label_encoder.classes_,\n    yticklabels=label_encoder.classes_\n)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n# Create a folder to save everything\n!pip install joblib\nsave_dir = \"./bert_lora_complaints\"\n\n# Save the PEFT (LoRA) adapter and base model\nclf.save_pretrained(save_dir)\ntokenizer.save_pretrained(save_dir)\n\n# Optional: save label encoder for use during inference\nimport joblib\njoblib.dump(label_encoder, f\"{save_dir}/label_encoder.pkl\")\n\nprint(f\"Model and tokenizer saved to {save_dir}\")\n\n\n# # ────────────────────────────────────────────────────────────────────────────────\n# # 11. Tiny LLM for email response generation","metadata":{"_uuid":"71875cc8-ffd6-4a19-9be1-c3c677f94a9c","_cell_guid":"49d8dc76-9b07-4f07-8216-9af7eb128584","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-11T19:46:57.133207Z","iopub.execute_input":"2025-05-11T19:46:57.133505Z","iopub.status.idle":"2025-05-11T20:10:53.419510Z","shell.execute_reply.started":"2025-05-11T19:46:57.133483Z","shell.execute_reply":"2025-05-11T20:10:53.418623Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a folder to save everything\n!pip install joblib\nsave_dir = \"./bert_lora_complaints\"\n\n# Save the PEFT (LoRA) adapter and base model\nclf.save_pretrained(save_dir)\ntokenizer.save_pretrained(save_dir)\n\n# Optional: save label encoder for use during inference\nimport joblib\njoblib.dump(label_encoder, f\"{save_dir}/label_encoder.pkl\")\n\nprint(f\"Model and tokenizer saved to {save_dir}\")","metadata":{"_uuid":"ab9ee61d-9642-44d8-bccf-2dbcef645a30","_cell_guid":"63b66e1d-fca3-4b52-91ae-20dfdab492b6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-11T19:38:55.191877Z","iopub.execute_input":"2025-05-11T19:38:55.192193Z","iopub.status.idle":"2025-05-11T19:38:58.430603Z","shell.execute_reply.started":"2025-05-11T19:38:55.192167Z","shell.execute_reply":"2025-05-11T19:38:58.429624Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n\ntest_texts = test_df[\"narrative\"].fillna(\"\").astype(str).tolist()\ntest_labels = label_encoder.inverse_transform(preds)\n\n# ─── 12. Load TinyLLaMA for email generation ────────────────────────────────\ngen_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\nbnb_llama = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# 12a. Tokenizer & Model (assign all layers to cuda:0)\nllama_tokenizer = AutoTokenizer.from_pretrained(gen_model_id)\nllama_model = AutoModelForCausalLM.from_pretrained(\n    gen_model_id,\n    quantization_config=bnb_llama,\n    device_map={\"\": 0},           # map entire model to GPU 0\n    low_cpu_mem_usage=True,\n)\nllama_model.to(device)\n\n# 12b. Text-generation pipeline (drop explicit `device` argument)\nllama_pipe = pipeline(\n    \"text-generation\",\n    model=llama_model,\n    tokenizer=llama_tokenizer\n)\n\n# ─── 12b. Text-generation pipeline (unchanged) ─────────────────────────────\nllama_pipe = pipeline(\n    \"text-generation\",\n    model=llama_model,\n    tokenizer=llama_tokenizer\n)\n\nfrom transformers import pipeline\n\n# ─── Sentiment analysis pipeline ─────────────────────────────────────────────\nsentiment_pipe = pipeline(\n    \"sentiment-analysis\",\n    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n    device=0  # uses cuda:0\n)\n\n# ─── 12c. Helper to generate empathetic responses ─────────────────────────────\ndef generate_email_response_llama(category: str, message: str) -> str:\n    # 1) Analyze customer sentiment\n    sentiment = sentiment_pipe(message)[0]\n    label = sentiment[\"label\"].lower()\n    score = sentiment[\"score\"]\n    \n    # 2) Choose tone based on sentiment\n    if label == \"negative\":\n        tone_instruction = \"Use an empathetic, apologetic tone that acknowledges the customer's frustration.\"\n    else:\n        tone_instruction = \"Use a warm, friendly tone that thanks the customer for their feedback.\"\n    \n    # 3) Build prompt\n    prompt = (\n        \"[INST] You are a helpful customer support assistant.\\n\\n\"\n        f\"Complaint Category: {category}\\n\"\n        f\"Customer Message: {message}\\n\\n\"\n        f\"Customer Sentiment: {label} (confidence {score:.2f}). {tone_instruction}\\n\\n\"\n        \"Write a professional email response to the customer. [/INST]\\n\"\n    )\n    \n    # 4) Generate, returning only the new text\n    out = llama_pipe(\n        prompt,\n        max_new_tokens=150,\n        do_sample=True,\n        top_p=0.9,\n        temperature=0.7,\n        truncation=True,\n        return_full_text=False\n    )\n    return out[0][\"generated_text\"].strip()\n\n# ─── 13. Generate on first 5 test cases ────────────────────────────────────\ntest_texts  = test_df[\"narrative\"].fillna(\"\").astype(str).tolist()\ntest_labels = label_encoder.inverse_transform(preds)\n\nfor txt, cat in zip(test_texts[:5], test_labels[:5]):\n    print(\"⟶ Complaint:\", txt)\n    print(\"⟶ Category :\", cat)\n    print(\"⟶ Response :\", generate_email_response_llama(cat, txt))\n    print(\"-\" * 80)\n\n# ─── 12c. Helper to generate responses (use max_new_tokens instead of max_length) ────\ndef generate_email_response_llama(category: str, message: str) -> str:\n    sentiment = sentiment_pipe(message)[0]\n    label = sentiment[\"label\"].lower()\n\n    # Simplified tone suggestion\n    if label == \"negative\":\n        tone_instruction = (\n            \"Acknowledge the customer's concern respectfully and offer a clear resolution path. \"\n            \"Avoid excessive emotion. Be concise and professional.\"\n        )\n    else:\n        tone_instruction = (\n            \"Thank the customer for their message and respond in a helpful, clear, and professional tone.\"\n        )\n\n    prompt = (\n        \"[INST] You are a professional customer support agent. \"\n        f\"Complaint Category: {category}\\n\"\n        f\"Customer Message: {message}\\n\\n\"\n        f\"{tone_instruction}\\n\\n\"\n        \"Write a short and professional email response. Avoid repeating the prompt. [/INST]\\n\"\n    )\n\n    out = llama_pipe(\n        prompt,\n        max_new_tokens=120,          # shorter response\n        do_sample=True,\n        top_p=0.85,                  # reduce randomness slightly\n        temperature=0.6,             # lower temperature for less emotional tone\n        truncation=True,\n        return_full_text=False\n    )\n    return out[0][\"generated_text\"].strip()\n\n\n# ─── 13. Generate on first 5 test cases ────────────────────────────────────\ntest_texts = test_df[\"narrative\"].fillna(\"\").astype(str).tolist()\ntest_labels = label_encoder.inverse_transform(preds)\n\nfor txt, cat in zip(test_texts[:5], test_labels[:5]):\n    print(\"⟶ Complaint:\", txt)\n    print(\"⟶ Category:\", cat)\n    print(\"⟶ LLaMA-Response:\", generate_email_response_llama(cat, txt))\n    print(\"-\" * 80)\n\n\n# ─── 12. Load TinyLLaMA for email generation ────────────────────────────────\n# gen_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\n# bnb_llama = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.bfloat16\n# )\n\n# # 12a. Tokenizer & Model\n# llama_tokenizer = AutoTokenizer.from_pretrained(gen_model_id)\n# llama_model = AutoModelForCausalLM.from_pretrained(\n#     gen_model_id,\n#     quantization_config=bnb_llama,\n#     low_cpu_mem_usage=True,\n# )\n# llama_model.to(device)\n\n# # 12b. Text-generation pipeline\n# llama_pipe = pipeline(\n#     \"text-generation\",\n#     model=llama_model,\n#     tokenizer=llama_tokenizer,\n#     device=0 if torch.cuda.is_available() else -1,\n# )\n\n# # 12c. Helper to generate responses\n# def generate_email_response_llama(category: str, message: str) -> str:\n#     prompt = (\n#         \"[INST] You are a helpful customer support assistant.\\n\\n\"\n#         f\"Complaint Category: {category}\\n\"\n#         f\"Customer Message: {message}\\n\\n\"\n#         \"Write a professional email response to the customer. [/INST]\"\n#     )\n#     out = llama_pipe(\n#         prompt,\n#         max_length=200,\n#         do_sample=True,\n#         top_p=0.9,\n#         temperature=0.7,\n#     )[0][\"generated_text\"]\n#     # strip off the prompt echo if necessary\n#     return out.split(\"[INST]\")[-1].strip()\n\n# # ─── 13. Generate on first 5 test cases ────────────────────────────────────\n# # (assuming you still have `test_texts` and `test_labels` from above)\n# for txt, cat in zip(test_texts[:5], test_labels[:5]):\n#     print(\"⟶ Complaint:\", txt)\n#     print(\"⟶ Category:\", cat)\n#     print(\"⟶ LLaMA-Response:\", generate_email_response_llama(cat, txt))\n#     print(\"-\" * 80)","metadata":{"_uuid":"f2fc5ca5-5f94-4556-a322-ceecdaa0f765","_cell_guid":"1372b5cf-0738-46d7-bcfd-6f7f9a6dc75c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-05-11T20:31:41.694047Z","iopub.execute_input":"2025-05-11T20:31:41.694571Z","iopub.status.idle":"2025-05-11T20:32:33.239805Z","shell.execute_reply.started":"2025-05-11T20:31:41.694547Z","shell.execute_reply":"2025-05-11T20:32:33.239073Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}